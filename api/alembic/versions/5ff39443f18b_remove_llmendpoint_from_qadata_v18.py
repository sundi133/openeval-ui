"""remove LLMEndpoint from QAData v18

Revision ID: 5ff39443f18b
Revises: 53477dacb72c
Create Date: 2023-12-23 15:18:26.748072

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "5ff39443f18b"
down_revision: Union[str, None] = "53477dacb72c"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "datasets",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("gen_id", sa.String(), nullable=True),
        sa.Column("name", sa.String(), nullable=True),
        sa.Column("userid", sa.String(), nullable=True),
        sa.Column("orgid", sa.String(), nullable=True),
        sa.Column("type", sa.String(), nullable=True),
        sa.Column("chat_type", sa.String(), nullable=True),
        sa.Column("sample_size", sa.Integer(), nullable=True),
        sa.Column("number_of_questions", sa.Integer(), nullable=True),
        sa.Column("reference_chunk", sa.String(), nullable=True),
        sa.Column("reference_chunk_max_distance", sa.Integer(), nullable=True),
        sa.Column("chunk_size", sa.Integer(), nullable=True),
        sa.Column("ts", sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_datasets_gen_id"), "datasets", ["gen_id"], unique=False)
    op.create_index(op.f("ix_datasets_id"), "datasets", ["id"], unique=False)
    op.create_index(op.f("ix_datasets_name"), "datasets", ["name"], unique=False)
    op.create_table(
        "llm_endpoints",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("userid", sa.String(), nullable=True),
        sa.Column("orgid", sa.String(), nullable=True),
        sa.Column("name", sa.String(), nullable=True),
        sa.Column("endpoint_url", sa.String(), nullable=True),
        sa.Column("ts", sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_llm_endpoints_id"), "llm_endpoints", ["id"], unique=False)
    op.create_index(
        op.f("ix_llm_endpoints_name"), "llm_endpoints", ["name"], unique=False
    )
    op.create_table(
        "qa_data",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("userid", sa.String(), nullable=True),
        sa.Column("orgid", sa.String(), nullable=True),
        sa.Column("dataset_id", sa.Integer(), nullable=True),
        sa.Column("ts", sa.DateTime(), nullable=True),
        sa.Column("chat_messages", sa.JSON(), nullable=True),
        sa.ForeignKeyConstraint(
            ["dataset_id"],
            ["datasets.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_qa_data_id"), "qa_data", ["id"], unique=False)
    op.create_table(
        "evaluations",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("userid", sa.String(), nullable=True),
        sa.Column("orgid", sa.String(), nullable=True),
        sa.Column("dataset_id", sa.Integer(), nullable=True),
        sa.Column("llm_endpoint_id", sa.Integer(), nullable=True),
        sa.Column("qa_data_id", sa.Integer(), nullable=True),
        sa.Column("ts", sa.DateTime(), nullable=True),
        sa.Column("score", sa.Float(), nullable=True),
        sa.ForeignKeyConstraint(
            ["dataset_id"],
            ["datasets.id"],
        ),
        sa.ForeignKeyConstraint(
            ["llm_endpoint_id"],
            ["llm_endpoints.id"],
        ),
        sa.ForeignKeyConstraint(
            ["qa_data_id"],
            ["qa_data.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_evaluations_id"), "evaluations", ["id"], unique=False)
    op.drop_index("ix_llm_endpoint_id", table_name="llm_endpoint")
    op.drop_index("ix_llm_endpoint_name", table_name="llm_endpoint")
    op.drop_table("llm_endpoint")
    op.drop_index("ix_dataset_gen_id", table_name="dataset")
    op.drop_index("ix_dataset_id", table_name="dataset")
    op.drop_index("ix_dataset_name", table_name="dataset")
    op.drop_table("dataset")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "dataset",
        sa.Column("id", sa.INTEGER(), autoincrement=True, nullable=False),
        sa.Column("gen_id", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("name", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("userid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("orgid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("type", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("chat_type", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("sample_size", sa.INTEGER(), autoincrement=False, nullable=True),
        sa.Column(
            "number_of_questions", sa.INTEGER(), autoincrement=False, nullable=True
        ),
        sa.Column("reference_chunk", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "reference_chunk_max_distance",
            sa.INTEGER(),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("chunk_size", sa.INTEGER(), autoincrement=False, nullable=True),
        sa.Column("ts", postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
        sa.PrimaryKeyConstraint("id", name="dataset_pkey"),
    )
    op.create_index("ix_dataset_name", "dataset", ["name"], unique=False)
    op.create_index("ix_dataset_id", "dataset", ["id"], unique=False)
    op.create_index("ix_dataset_gen_id", "dataset", ["gen_id"], unique=False)
    op.create_table(
        "llm_endpoint",
        sa.Column("id", sa.INTEGER(), autoincrement=True, nullable=False),
        sa.Column("userid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("orgid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("name", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("endpoint_url", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("ts", postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
        sa.PrimaryKeyConstraint("id", name="llm_endpoint_pkey"),
    )
    op.create_index("ix_llm_endpoint_name", "llm_endpoint", ["name"], unique=False)
    op.create_index("ix_llm_endpoint_id", "llm_endpoint", ["id"], unique=False)
    op.drop_index(op.f("ix_evaluations_id"), table_name="evaluations")
    op.drop_table("evaluations")
    op.drop_index(op.f("ix_qa_data_id"), table_name="qa_data")
    op.drop_table("qa_data")
    op.drop_index(op.f("ix_llm_endpoints_name"), table_name="llm_endpoints")
    op.drop_index(op.f("ix_llm_endpoints_id"), table_name="llm_endpoints")
    op.drop_table("llm_endpoints")
    op.drop_index(op.f("ix_datasets_name"), table_name="datasets")
    op.drop_index(op.f("ix_datasets_id"), table_name="datasets")
    op.drop_index(op.f("ix_datasets_gen_id"), table_name="datasets")
    op.drop_table("datasets")
    # ### end Alembic commands ###
