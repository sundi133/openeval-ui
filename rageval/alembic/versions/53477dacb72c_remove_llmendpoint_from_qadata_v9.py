"""remove LLMEndpoint from QAData v9

Revision ID: 53477dacb72c
Revises: 6042b16de719
Create Date: 2023-12-23 14:36:08.842833

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "53477dacb72c"
down_revision: Union[str, None] = "6042b16de719"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "dataset",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("gen_id", sa.String(), nullable=True),
        sa.Column("name", sa.String(), nullable=True),
        sa.Column("userid", sa.String(), nullable=True),
        sa.Column("orgid", sa.String(), nullable=True),
        sa.Column("type", sa.String(), nullable=True),
        sa.Column("chat_type", sa.String(), nullable=True),
        sa.Column("sample_size", sa.Integer(), nullable=True),
        sa.Column("number_of_questions", sa.Integer(), nullable=True),
        sa.Column("reference_chunk", sa.String(), nullable=True),
        sa.Column("reference_chunk_max_distance", sa.Integer(), nullable=True),
        sa.Column("chunk_size", sa.Integer(), nullable=True),
        sa.Column("ts", sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_dataset_gen_id"), "dataset", ["gen_id"], unique=False)
    op.create_index(op.f("ix_dataset_id"), "dataset", ["id"], unique=False)
    op.create_index(op.f("ix_dataset_name"), "dataset", ["name"], unique=False)
    op.create_table(
        "llm_endpoint",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("userid", sa.String(), nullable=True),
        sa.Column("orgid", sa.String(), nullable=True),
        sa.Column("name", sa.String(), nullable=True),
        sa.Column("endpoint_url", sa.String(), nullable=True),
        sa.Column("ts", sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_llm_endpoint_id"), "llm_endpoint", ["id"], unique=False)
    op.create_index(
        op.f("ix_llm_endpoint_name"), "llm_endpoint", ["name"], unique=False
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, "qa_data", type_="foreignkey")
    op.create_foreign_key(
        "qa_data_dataset_id_fkey", "qa_data", "datasets", ["dataset_id"], ["id"]
    )
    op.drop_constraint(None, "evaluations", type_="foreignkey")
    op.drop_constraint(None, "evaluations", type_="foreignkey")
    op.create_foreign_key(
        "evaluations_llm_endpoint_id_fkey",
        "evaluations",
        "llm_endpoints",
        ["llm_endpoint_id"],
        ["id"],
    )
    op.create_foreign_key(
        "evaluations_dataset_id_fkey", "evaluations", "datasets", ["dataset_id"], ["id"]
    )
    op.create_table(
        "llm_endpoints",
        sa.Column("id", sa.INTEGER(), autoincrement=True, nullable=False),
        sa.Column("name", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("endpoint_url", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("userid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("orgid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("ts", postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
        sa.PrimaryKeyConstraint("id", name="llm_endpoints_pkey"),
    )
    op.create_index("ix_llm_endpoints_name", "llm_endpoints", ["name"], unique=False)
    op.create_index("ix_llm_endpoints_id", "llm_endpoints", ["id"], unique=False)
    op.create_table(
        "datasets",
        sa.Column("id", sa.INTEGER(), autoincrement=True, nullable=False),
        sa.Column("name", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("userid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("orgid", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("type", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("chat_type", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("sample_size", sa.INTEGER(), autoincrement=False, nullable=True),
        sa.Column(
            "number_of_questions", sa.INTEGER(), autoincrement=False, nullable=True
        ),
        sa.Column("reference_chunk", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "reference_chunk_max_distance",
            sa.INTEGER(),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("chunk_size", sa.INTEGER(), autoincrement=False, nullable=True),
        sa.Column("gen_id", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("ts", postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
        sa.PrimaryKeyConstraint("id", name="datasets_pkey"),
    )
    op.create_index("ix_datasets_name", "datasets", ["name"], unique=False)
    op.create_index("ix_datasets_id", "datasets", ["id"], unique=False)
    op.create_index("ix_datasets_gen_id", "datasets", ["gen_id"], unique=False)
    op.drop_index(op.f("ix_llm_endpoint_name"), table_name="llm_endpoint")
    op.drop_index(op.f("ix_llm_endpoint_id"), table_name="llm_endpoint")
    op.drop_table("llm_endpoint")
    op.drop_index(op.f("ix_dataset_name"), table_name="dataset")
    op.drop_index(op.f("ix_dataset_id"), table_name="dataset")
    op.drop_index(op.f("ix_dataset_gen_id"), table_name="dataset")
    op.drop_table("dataset")
    # ### end Alembic commands ###
